<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>《转载》Tensorflow一些常用基本概念与函数 | Kang Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="1、tensorflow的基本运作为了快速的熟悉TensorFlow编程，下面从一段简单的代码开始： 123456789101112import tensorflow as tf #定义‘符号’变量，也称为占位符 a = tf.placeholder(&quot;float&quot;) b = tf.placeholder(&quot;float&quot;) y = tf.mul(a, b) #构造一个op节点 sess = tf.">
<meta name="keywords" content="tensorflow,ML">
<meta property="og:type" content="article">
<meta property="og:title" content="《转载》Tensorflow一些常用基本概念与函数">
<meta property="og:url" content="http://blog.kangyufei.net/2018/03/20/tensorflow/tensorflow-api-common-func/index.html">
<meta property="og:site_name" content="Kang Blog">
<meta property="og:description" content="1、tensorflow的基本运作为了快速的熟悉TensorFlow编程，下面从一段简单的代码开始： 123456789101112import tensorflow as tf #定义‘符号’变量，也称为占位符 a = tf.placeholder(&quot;float&quot;) b = tf.placeholder(&quot;float&quot;) y = tf.mul(a, b) #构造一个op节点 sess = tf.">
<meta property="og:locale" content="zh">
<meta property="og:image" content="http://img.blog.csdn.net/20160808174705034">
<meta property="og:updated_time" content="2018-03-20T04:32:54.439Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="《转载》Tensorflow一些常用基本概念与函数">
<meta name="twitter:description" content="1、tensorflow的基本运作为了快速的熟悉TensorFlow编程，下面从一段简单的代码开始： 123456789101112import tensorflow as tf #定义‘符号’变量，也称为占位符 a = tf.placeholder(&quot;float&quot;) b = tf.placeholder(&quot;float&quot;) y = tf.mul(a, b) #构造一个op节点 sess = tf.">
<meta name="twitter:image" content="http://img.blog.csdn.net/20160808174705034">
  
    <link rel="alternative" href="/atom.xml" title="Kang Blog" type="application/atom+xml">
  
  
  
      <link rel="stylesheet" href="//cdn.bootcss.com/animate.css/3.5.0/animate.min.css">
  
  <link rel="stylesheet" href="/css/style.css">
  <link rel="stylesheet" href="/font-awesome/css/font-awesome.min.css">
  <link rel="apple-touch-icon" href="/apple-touch-icon.png">
  
  
      <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  
  <!-- 加载特效 -->
    <script src="/js/pace.js"></script>
    <link href="/css/pace/pace-theme-flash.css" rel="stylesheet" />
  <script>
      var yiliaConfig = {
          rootUrl: '/',
          fancybox: true,
          animate: true,
          isHome: false,
          isPost: true,
          isArchive: false,
          isTag: false,
          isCategory: false,
          open_in_new: false
      }
  </script>
</head>
<body>
  <div id="container">
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
    <header id="header" class="inner">
        <a href="/" class="profilepic">
            
            <img lazy-src="/img/head.jpg" class="js-avatar">
            
        </a>

        <hgroup>
          <h1 class="header-author"><a href="/" title="Hi Mate">Kang</a></h1>
        </hgroup>

        
        
        
            <div id="switch-btn" class="switch-btn">
                <div class="icon">
                    <div class="icon-ctn">
                        <div class="icon-wrap icon-house" data-idx="0">
                            <div class="birdhouse"></div>
                            <div class="birdhouse_holes"></div>
                        </div>
                        <div class="icon-wrap icon-ribbon hide" data-idx="1">
                            <div class="ribbon"></div>
                        </div>
                        
                        <div class="icon-wrap icon-link hide" data-idx="2">
                            <div class="loopback_l"></div>
                            <div class="loopback_r"></div>
                        </div>
                        
                        
                        <div class="icon-wrap icon-me hide" data-idx="3">
                            <div class="user"></div>
                            <div class="shoulder"></div>
                        </div>
                        
                    </div>
                    
                </div>
                <div class="tips-box hide">
                    <div class="tips-arrow"></div>
                    <ul class="tips-inner">
                        <li>菜单</li>
                        <li>标签</li>
                        
                        <li>友情链接</li>
                        
                        
                        <li>关于我</li>
                        
                    </ul>
                </div>
            </div>
        

        <div id="switch-area" class="switch-area">
            <div class="switch-wrap">
                <section class="switch-part switch-part1">
                    <nav class="header-menu">
                        <ul>
                        
                            <li><a href="/">首页</a></li>
                        
                            <li><a href="/tags">标签</a></li>
                        
                            <li><a href="/categories">分类</a></li>
                        
                        </ul>
                    </nav>
                    <nav class="header-nav">
                        <ul class="social">
                            
                                <a class="fl github" target="_blank" href="https://github.com/Yufeikang" title="github">github</a>
                            
                                <a class="fl linkedin" target="_blank" href="#" title="linkedin">linkedin</a>
                            
                        </ul>
                    </nav>
                </section>
                
                
                <section class="switch-part switch-part2">
                    <div class="widget tagcloud" id="js-tagcloud">
                        <a href="/tags/ML/" style="font-size: 15px;">ML</a> <a href="/tags/android/" style="font-size: 10px;">android</a> <a href="/tags/javascript/" style="font-size: 10px;">javascript</a> <a href="/tags/linux/" style="font-size: 10px;">linux</a> <a href="/tags/node/" style="font-size: 10px;">node</a> <a href="/tags/python/" style="font-size: 20px;">python</a> <a href="/tags/rabbitmq/" style="font-size: 10px;">rabbitmq</a> <a href="/tags/tensorflow/" style="font-size: 15px;">tensorflow</a> <a href="/tags/网络/" style="font-size: 10px;">网络</a>
                    </div>
                </section>
                
                
                
                <section class="switch-part switch-part3">
                    <div id="js-friends">
                    
                      <a target="_blank" class="main-nav-link switch-friends-link" href="http://yufeikang.github.io/">name</a>
                    
                    </div>
                </section>
                

                
                
                <section class="switch-part switch-part4">
                
                    <div id="js-aboutme">生命不息，折腾不止</div>
                </section>
                
            </div>
        </div>
    </header>                
</div>
    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
      <div class="overlay">
          <div class="slider-trigger"></div>
          <h1 class="header-author js-mobile-header hide"><a href="/" title="Me">Kang</a></h1>
      </div>
    <div class="intrude-less">
        <header id="header" class="inner">
            <a href="/" class="profilepic">
                
                    <img lazy-src="/img/head.jpg" class="js-avatar">
                
            </a>
            <hgroup>
              <h1 class="header-author"><a href="/" title="Me">Kang</a></h1>
            </hgroup>
            
            <nav class="header-menu">
                <ul>
                
                    <li><a href="/">首页</a></li>
                
                    <li><a href="/tags">标签</a></li>
                
                    <li><a href="/categories">分类</a></li>
                
                <div class="clearfix"></div>
                </ul>
            </nav>
            <nav class="header-nav">
                <div class="social">
                    
                        <a class="github" target="_blank" href="https://github.com/Yufeikang" title="github">github</a>
                    
                        <a class="linkedin" target="_blank" href="#" title="linkedin">linkedin</a>
                    
                </div>
            </nav>
        </header>                
    </div>
</nav>
      <div class="body-wrap"><article id="post-tensorflow/tensorflow-api-common-func" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/03/20/tensorflow/tensorflow-api-common-func/" class="article-date">
      <time datetime="2018-03-20T04:20:07.000Z" itemprop="datePublished">2018-03-20</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      《转载》Tensorflow一些常用基本概念与函数
    </h1>
  

      </header>
      
      <div class="article-info article-info-post">
        

        
    <div class="article-tag tagcloud">
        <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/ML/">ML</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/tensorflow/">tensorflow</a></li></ul>
    </div>

        <div class="clearfix"></div>
      </div>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <h3 id="1、tensorflow的基本运作"><a href="#1、tensorflow的基本运作" class="headerlink" title="1、tensorflow的基本运作"></a>1、tensorflow的基本运作</h3><p>为了快速的熟悉TensorFlow编程，下面从一段简单的代码开始：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"> <span class="comment">#定义‘符号’变量，也称为占位符</span></span><br><span class="line"> a = tf.placeholder(<span class="string">"float"</span>)</span><br><span class="line"> b = tf.placeholder(<span class="string">"float"</span>)</span><br><span class="line"></span><br><span class="line"> y = tf.mul(a, b) <span class="comment">#构造一个op节点</span></span><br><span class="line"></span><br><span class="line"> sess = tf.Session()<span class="comment">#建立会话</span></span><br><span class="line"> <span class="comment">#运行会话，输入数据，并计算节点，同时打印结果</span></span><br><span class="line"> <span class="keyword">print</span> sess.run(y, feed_dict=&#123;a: <span class="number">3</span>, b: <span class="number">3</span>&#125;)</span><br><span class="line"> <span class="comment"># 任务完成, 关闭会话.</span></span><br><span class="line"> sess.close()<span class="number">123456789101112</span></span><br></pre></td></tr></table></figure>
<p>其中tf.mul(a, b)函数便是tf的一个基本的算数运算，接下来介绍跟多的相关函数。</p>
<h3 id="2、tf函数"><a href="#2、tf函数" class="headerlink" title="2、tf函数"></a>2、tf函数</h3><blockquote>
<p>TensorFlow 将图形定义转换成分布式执行的操作, 以充分利用可用的计算资源(如 CPU 或 GPU。一般你不需要显式指定使用 CPU 还是 GPU, TensorFlow 能自动检测。如果检测到 GPU, TensorFlow 会尽可能地利用找到的第一个 GPU 来执行操作.<br>并行计算能让代价大的算法计算加速执行，TensorFlow也在实现上对复杂操作进行了有效的改进。大部分核相关的操作都是设备相关的实现，比如GPU。下面是一些重要的操作/核：</p>
</blockquote>
<table>
<thead>
<tr>
<th>操作组</th>
<th>操作</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maths</td>
<td>Add, Sub, Mul, Div, Exp, Log, Greater, Less, Equal</td>
</tr>
<tr>
<td>Array</td>
<td>Concat, Slice, Split, Constant, Rank, Shape, Shuffle</td>
</tr>
<tr>
<td>Matrix</td>
<td>MatMul, MatrixInverse, MatrixDeterminant</td>
</tr>
<tr>
<td>Neuronal Network</td>
<td>SoftMax, Sigmoid, ReLU, Convolution2D, MaxPool</td>
</tr>
<tr>
<td>Checkpointing</td>
<td>Save, Restore</td>
</tr>
<tr>
<td>Queues and syncronizations</td>
<td>Enqueue, Dequeue, MutexAcquire, MutexRelease</td>
</tr>
<tr>
<td>Flow control</td>
<td>Merge, Switch, Enter, Leave, NextIteration</td>
</tr>
</tbody>
</table>
<h4 id="TensorFlow的算术操作如下："><a href="#TensorFlow的算术操作如下：" class="headerlink" title="TensorFlow的算术操作如下："></a>TensorFlow的算术操作如下：</h4><table>
<thead>
<tr>
<th>操作</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>tf.add(x, y, name=None)</td>
<td>求和</td>
</tr>
<tr>
<td>tf.sub(x, y, name=None)</td>
<td>减法</td>
</tr>
<tr>
<td>tf.mul(x, y, name=None)</td>
<td>乘法</td>
</tr>
<tr>
<td>tf.div(x, y, name=None)</td>
<td>除法</td>
</tr>
<tr>
<td>tf.mod(x, y, name=None)</td>
<td>取模</td>
</tr>
<tr>
<td>tf.abs(x, name=None)</td>
<td>求绝对值</td>
</tr>
<tr>
<td>tf.neg(x, name=None)</td>
<td>取负 (y = -x).</td>
</tr>
<tr>
<td>tf.sign(x, name=None)</td>
<td>返回符号 y = sign(x) = -1 if x &lt; 0; 0 if x == 0; 1 if x &gt; 0.</td>
</tr>
<tr>
<td>tf.inv(x, name=None)</td>
<td>取反</td>
</tr>
<tr>
<td>tf.square(x, name=None)</td>
<td>计算平方 (y = x * x = x^2).</td>
</tr>
<tr>
<td>tf.round(x, name=None)</td>
<td>舍入最接近的整数# ‘a’ is [0.9, 2.5, 2.3, -4.4]tf.round(a) ==&gt; [ 1.0, 3.0, 2.0, -4.0 ]</td>
</tr>
<tr>
<td>tf.sqrt(x, name=None)</td>
<td>开根号 (y = \sqrt{x} = x^{1/2}).</td>
</tr>
<tr>
<td>tf.pow(x, y, name=None)</td>
<td>幂次方 # tensor ‘x’ is [[2, 2], [3, 3]]# tensor ‘y’ is [[8, 16], [2, 3]]tf.pow(x, y) ==&gt; [[256, 65536], [9, 27]]</td>
</tr>
<tr>
<td>tf.exp(x, name=None)</td>
<td>计算e的次方</td>
</tr>
<tr>
<td>tf.log(x, name=None)</td>
<td>计算log，一个输入计算e的ln，两输入以第二输入为底</td>
</tr>
<tr>
<td>tf.maximum(x, y, name=None)</td>
<td>返回最大值 (x &gt; y ? x : y)</td>
</tr>
<tr>
<td>tf.minimum(x, y, name=None)</td>
<td>返回最小值 (x &lt; y ? x : y)</td>
</tr>
<tr>
<td>tf.cos(x, name=None)</td>
<td>三角函数cosine</td>
</tr>
<tr>
<td>tf.sin(x, name=None)</td>
<td>三角函数sine</td>
</tr>
<tr>
<td>tf.tan(x, name=None)</td>
<td>三角函数tan</td>
</tr>
<tr>
<td>tf.atan(x, name=None)</td>
<td>三角函数ctan</td>
</tr>
</tbody>
</table>
<hr>
<h4 id="张量操作Tensor-Transformations"><a href="#张量操作Tensor-Transformations" class="headerlink" title="张量操作Tensor Transformations"></a>张量操作Tensor Transformations</h4><ul>
<li>数据类型转换Casting</li>
</ul>
<table>
<thead>
<tr>
<th>操作</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>tf.string_to_number(string_tensor, out_type=None, name=None)</td>
<td>字符串转为数字</td>
</tr>
<tr>
<td>tf.to_double(x, name=’ToDouble’)</td>
<td>转为64位浮点类型–float64</td>
</tr>
<tr>
<td>tf.to_float(x, name=’ToFloat’)</td>
<td>转为32位浮点类型–float32</td>
</tr>
<tr>
<td>tf.to_int32(x, name=’ToInt32’)</td>
<td>转为32位整型–int32</td>
</tr>
<tr>
<td>tf.to_int64(x, name=’ToInt64’)</td>
<td>转为64位整型–int64</td>
</tr>
<tr>
<td>tf.cast(x, dtype, name=None)</td>
<td>将x或者x.values转换为dtype# tensor <code>a</code> is [1.8, 2.2], dtype=tf.floattf.cast(a, tf.int32) ==&gt; [1, 2] # dtype=tf.int32</td>
</tr>
<tr>
<td></td>
</tr>
</tbody>
</table>
<ul>
<li>形状操作Shapes and Shaping</li>
</ul>
<table>
<thead>
<tr>
<th>操作</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>tf.shape(input, name=None)</td>
<td>返回数据的shape# ‘t’ is [[[1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]]]shape(t) ==&gt; [2, 2, 3]</td>
</tr>
<tr>
<td>tf.size(input, name=None)</td>
<td>返回数据的元素数量# ‘t’ is [[[1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]]]]size(t) ==&gt; 12</td>
</tr>
<tr>
<td>tf.rank(input, name=None)</td>
<td>返回tensor的rank注意：此rank不同于矩阵的rank，tensor的rank表示一个tensor需要的索引数目来唯一表示任何一个元素也就是通常所说的 “order”, “degree”或”ndims”#’t’ is [[[1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]]]# shape of tensor ‘t’ is [2, 2, 3]rank(t) ==&gt; 3</td>
</tr>
<tr>
<td>tf.reshape(tensor, shape, name=None)</td>
<td>改变tensor的形状# tensor ‘t’ is [1, 2, 3, 4, 5, 6, 7, 8, 9]# tensor ‘t’ has shape [9]reshape(t, [3, 3]) ==&gt; [[1, 2, 3],[4, 5, 6],[7, 8, 9]]#如果shape有元素[-1],表示在该维度打平至一维# -1 将自动推导得为 9:reshape(t, [2, -1]) ==&gt; [[1, 1, 1, 2, 2, 2, 3, 3, 3],[4, 4, 4, 5, 5, 5, 6, 6, 6]]</td>
</tr>
<tr>
<td>tf.expand_dims(input, dim, name=None)</td>
<td>插入维度1进入一个tensor中#该操作要求-1-input.dims()# ‘t’ is a tensor of shape [2]shape(expand_dims(t, 0)) ==&gt; [1, 2]shape(expand_dims(t, 1)) ==&gt; [2, 1]shape(expand_dims(t, -1)) ==&gt; [2, 1] &lt;= dim &lt;= input.dims()</td>
</tr>
</tbody>
</table>
<ul>
<li>切片与合并（Slicing and Joining）</li>
</ul>
<table>
<thead>
<tr>
<th>操作</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>tf.slice(input_, begin, size, name=None)</td>
<td>对tensor进行切片操作其中size[i] = input.dim_size(i) - begin[i]该操作要求 0 &lt;= begin[i] &lt;= begin[i] + size[i] &lt;= Di for i in [0, n]#’input’ is #[[[1, 1, 1], [2, 2, 2]],[[3, 3, 3], [4, 4, 4]],[[5, 5, 5], [6, 6, 6]]]tf.slice(input, [1, 0, 0], [1, 1, 3]) ==&gt; [[[3, 3, 3]]]tf.slice(input, [1, 0, 0], [1, 2, 3]) ==&gt; [[[3, 3, 3],[4, 4, 4]]]tf.slice(input, [1, 0, 0], [2, 1, 3]) ==&gt; [[[3, 3, 3]],[[5, 5, 5]]]</td>
</tr>
<tr>
<td>tf.split(split_dim, num_split, value, name=’split’)</td>
<td>沿着某一维度将tensor分离为num_split tensors# ‘value’ is a tensor with shape [5, 30]# Split ‘value’ into 3 tensors along dimension 1split0, split1, split2 = tf.split(1, 3, value)tf.shape(split0) ==&gt; [5, 10]</td>
</tr>
<tr>
<td>tf.concat(concat_dim, values, name=’concat’)</td>
<td>沿着某一维度连结tensort1 = [[1, 2, 3], [4, 5, 6]]t2 = [[7, 8, 9], [10, 11, 12]]tf.concat(0, [t1, t2]) ==&gt; [[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]]tf.concat(1, [t1, t2]) ==&gt; [[1, 2, 3, 7, 8, 9], [4, 5, 6, 10, 11, 12]]如果想沿着tensor一新轴连结打包,那么可以：tf.concat(axis, [tf.expand_dims(t, axis) for t in tensors])等同于tf.pack(tensors, axis=axis)</td>
</tr>
<tr>
<td>tf.pack(values, axis=0, name=’pack’)</td>
<td>将一系列rank-R的tensor打包为一个rank-(R+1)的tensor# ‘x’ is [1, 4], ‘y’ is [2, 5], ‘z’ is [3, 6]pack([x, y, z]) =&gt; [[1, 4], [2, 5], [3, 6]] # 沿着第一维packpack([x, y, z], axis=1) =&gt; [[1, 2, 3], [4, 5, 6]]等价于tf.pack([x, y, z]) = np.asarray([x, y, z])</td>
</tr>
<tr>
<td>tf.reverse(tensor, dims, name=None)</td>
<td>沿着某维度进行序列反转其中dim为列表，元素为bool型，size等于rank(tensor)# tensor ‘t’ is [[[[ 0, 1, 2, 3],#[ 4, 5, 6, 7],#[ 8, 9, 10, 11]],#[[12, 13, 14, 15],#[16, 17, 18, 19],#[20, 21, 22, 23]]]]# tensor ‘t’ shape is [1, 2, 3, 4]# ‘dims’ is [False, False, False, True]reverse(t, dims) ==&gt;[[[[ 3, 2, 1, 0],[ 7, 6, 5, 4],[ 11, 10, 9, 8]],[[15, 14, 13, 12],[19, 18, 17, 16],[23, 22, 21, 20]]]]</td>
</tr>
<tr>
<td>tf.transpose(a, perm=None, name=’transpose’)</td>
<td>调换tensor的维度顺序按照列表perm的维度排列调换tensor顺序，如为定义，则perm为(n-1…0)# ‘x’ is [[1 2 3],[4 5 6]]tf.transpose(x) ==&gt; [[1 4], [2 5],[3 6]]# Equivalentlytf.transpose(x, perm=[1, 0]) ==&gt; [[1 4],[2 5], [3 6]]</td>
</tr>
<tr>
<td>tf.gather(params, indices, validate_indices=None, name=None)</td>
<td>合并索引indices所指示params中的切片<img src="http://img.blog.csdn.net/20160808174705034" alt="tf.gather"></td>
</tr>
<tr>
<td>tf.one_hot(indices, depth, on_value=None, off_value=None, axis=None, dtype=None, name=None)</td>
<td>indices = [0, 2, -1, 1]depth = 3on_value = 5.0 off_value = 0.0 axis = -1 #Then output is [4 x 3]: output = [5.0 0.0 0.0] // one_hot(0) [0.0 0.0 5.0] // one_hot(2) [0.0 0.0 0.0] // one_hot(-1) [0.0 5.0 0.0] // one_hot(1)</td>
</tr>
</tbody>
</table>
<hr>
<h4 id="矩阵相关运算"><a href="#矩阵相关运算" class="headerlink" title="矩阵相关运算"></a>矩阵相关运算</h4><table>
<thead>
<tr>
<th>操作</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>tf.diag(diagonal, name=None)</td>
<td>返回一个给定对角值的对角tensor# ‘diagonal’ is [1, 2, 3, 4]tf.diag(diagonal) ==&gt; [[1, 0, 0, 0][0, 2, 0, 0][0, 0, 3, 0][0, 0, 0, 4]]</td>
</tr>
<tr>
<td>tf.diag_part(input, name=None)</td>
<td>功能与上面相反</td>
</tr>
<tr>
<td>tf.trace(x, name=None)</td>
<td>求一个2维tensor足迹，即对角值diagonal之和</td>
</tr>
<tr>
<td>tf.transpose(a, perm=None, name=’transpose’)</td>
<td>调换tensor的维度顺序按照列表perm的维度排列调换tensor顺序，如为定义，则perm为(n-1…0)# ‘x’ is [[1 2 3],[4 5 6]]tf.transpose(x) ==&gt; [[1 4], [2 5],[3 6]]# Equivalentlytf.transpose(x, perm=[1, 0]) ==&gt; [[1 4],[2 5], [3 6]]</td>
</tr>
<tr>
<td>tf.matmul(a, b, transpose_a=False, transpose_b=False, a_is_sparse=False, b_is_sparse=False, name=None)</td>
<td>矩阵相乘</td>
</tr>
<tr>
<td>tf.matrix_determinant(input, name=None)</td>
<td>返回方阵的行列式</td>
</tr>
<tr>
<td>tf.matrix_inverse(input, adjoint=None, name=None)</td>
<td>求方阵的逆矩阵，adjoint为True时，计算输入共轭矩阵的逆矩阵</td>
</tr>
<tr>
<td>tf.cholesky(input, name=None)</td>
<td>对输入方阵cholesky分解，即把一个对称正定的矩阵表示成一个下三角矩阵L和其转置的乘积的分解A=LL^T</td>
</tr>
<tr>
<td>tf.matrix_solve(matrix, rhs, adjoint=None, name=None)</td>
<td>求解tf.matrix_solve(matrix, rhs, adjoint=None, name=None)matrix为方阵shape为[M,M],rhs的shape为[M,K]，output为[M,K]</td>
</tr>
</tbody>
</table>
<hr>
<h4 id="复数操作"><a href="#复数操作" class="headerlink" title="复数操作"></a>复数操作</h4><table>
<thead>
<tr>
<th>操作</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>tf.complex(real, imag, name=None)</td>
<td>将两实数转换为复数形式# tensor ‘real’ is [2.25, 3.25]# tensor <code>imag</code> is [4.75, 5.75]tf.complex(real, imag) ==&gt; [[2.25 + 4.75j], [3.25 + 5.75j]]</td>
</tr>
<tr>
<td>tf.complex_abs(x, name=None)</td>
<td>计算复数的绝对值，即长度。# tensor ‘x’ is [[-2.25 + 4.75j], [-3.25 + 5.75j]]tf.complex_abs(x) ==&gt; [5.25594902, 6.60492229]</td>
</tr>
<tr>
<td>tf.conj(input, name=None)</td>
<td>计算共轭复数</td>
</tr>
<tr>
<td>tf.imag(input, name=None)tf.real(input, name=None)</td>
<td>提取复数的虚部和实部</td>
</tr>
<tr>
<td>tf.fft(input, name=None)</td>
<td>计算一维的离散傅里叶变换，输入数据类型为complex64</td>
</tr>
</tbody>
</table>
<hr>
<h4 id="归约计算-Reduction"><a href="#归约计算-Reduction" class="headerlink" title="归约计算(Reduction)"></a>归约计算(Reduction)</h4><table>
<thead>
<tr>
<th>操作</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>tf.reduce_sum(input_tensor, reduction_indices=None, keep_dims=False, name=None)</td>
<td>计算输入tensor元素的和，或者安照reduction_indices指定的轴进行求和# ‘x’ is [[1, 1, 1]# [1, 1, 1]]tf.reduce_sum(x) ==&gt; 6tf.reduce_sum(x, 0) ==&gt; [2, 2, 2]tf.reduce_sum(x, 1) ==&gt; [3, 3]tf.reduce_sum(x, 1, keep_dims=True) ==&gt; [[3], [3]]tf.reduce_sum(x, [0, 1]) ==&gt; 6</td>
</tr>
<tr>
<td>tf.reduce_prod(input_tensor, reduction_indices=None, keep_dims=False, name=None)</td>
<td>计算输入tensor元素的乘积，或者安照reduction_indices指定的轴进行求乘积</td>
</tr>
<tr>
<td>tf.reduce_min(input_tensor, reduction_indices=None, keep_dims=False, name=None)</td>
<td>求tensor中最小值</td>
</tr>
<tr>
<td>tf.reduce_max(input_tensor, reduction_indices=None, keep_dims=False, name=None)</td>
<td>求tensor中最大值</td>
</tr>
<tr>
<td>tf.reduce_mean(input_tensor, reduction_indices=None, keep_dims=False, name=None)</td>
<td>求tensor中平均值</td>
</tr>
<tr>
<td>tf.reduce_all(input_tensor, reduction_indices=None, keep_dims=False, name=None)</td>
<td>对tensor中各个元素求逻辑’与’# ‘x’ is # [[True, True]# [False, False]]tf.reduce_all(x) ==&gt; Falsetf.reduce_all(x, 0) ==&gt; [False, False]tf.reduce_all(x, 1) ==&gt; [True, False]</td>
</tr>
<tr>
<td>tf.reduce_any(input_tensor, reduction_indices=None, keep_dims=False, name=None)</td>
<td>对tensor中各个元素求逻辑’或’</td>
</tr>
<tr>
<td>tf.accumulate_n(inputs, shape=None, tensor_dtype=None, name=None)</td>
<td>计算一系列tensor的和# tensor ‘a’ is [[1, 2], [3, 4]]# tensor <code>b</code> is [[5, 0], [0, 6]]tf.accumulate_n([a, b, a]) ==&gt; [[7, 4], [6, 14]]</td>
</tr>
<tr>
<td>tf.cumsum(x, axis=0, exclusive=False, reverse=False, name=None)</td>
<td>求累积和tf.cumsum([a, b, c]) ==&gt; [a, a + b, a + b + c]tf.cumsum([a, b, c], exclusive=True) ==&gt; [0, a, a + b]tf.cumsum([a, b, c], reverse=True) ==&gt; [a + b + c, b + c, c]tf.cumsum([a, b, c], exclusive=True, reverse=True) ==&gt; [b + c, c, 0]</td>
</tr>
<tr>
<td></td>
</tr>
</tbody>
</table>
<hr>
<h4 id="分割-Segmentation"><a href="#分割-Segmentation" class="headerlink" title="分割(Segmentation)"></a>分割(Segmentation)</h4><table>
<thead>
<tr>
<th>操作</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>tf.segment_sum(data, segment_ids, name=None)</td>
<td>根据segment_ids的分段计算各个片段的和其中segment_ids为一个size与data第一维相同的tensor其中id为int型数据，最大id不大于sizec = tf.constant([[1,2,3,4], [-1,-2,-3,-4], [5,6,7,8]])tf.segment_sum(c, tf.constant([0, 0, 1]))==&gt;[[0 0 0 0] [5 6 7 8]]上面例子分为[0,1]两id,对相同id的data相应数据进行求和,并放入结果的相应id中，且segment_ids只升不降</td>
</tr>
<tr>
<td>tf.segment_prod(data, segment_ids, name=None)</td>
<td>根据segment_ids的分段计算各个片段的积</td>
</tr>
<tr>
<td>tf.segment_min(data, segment_ids, name=None)</td>
<td>根据segment_ids的分段计算各个片段的最小值</td>
</tr>
<tr>
<td>tf.segment_max(data, segment_ids, name=None)</td>
<td>根据segment_ids的分段计算各个片段的最大值</td>
</tr>
<tr>
<td>tf.segment_mean(data, segment_ids, name=None)</td>
<td>根据segment_ids的分段计算各个片段的平均值</td>
</tr>
<tr>
<td>tf.unsorted_segment_sum(data, segment_ids,num_segments, name=None)</td>
<td>与tf.segment_sum函数类似，不同在于segment_ids中id顺序可以是无序的</td>
</tr>
<tr>
<td>tf.sparse_segment_sum(data, indices, segment_ids, name=None)</td>
<td>输入进行稀疏分割求和c = tf.constant([[1,2,3,4], [-1,-2,-3,-4], [5,6,7,8]])# Select two rows, one segment.tf.sparse_segment_sum(c, tf.constant([0, 1]), tf.constant([0, 0])) ==&gt; [[0 0 0 0]]对原data的indices为[0,1]位置的进行分割，并按照segment_ids的分组进行求和</td>
</tr>
</tbody>
</table>
<hr>
<h4 id="序列比较与索引提取-Sequence-Comparison-and-Indexing"><a href="#序列比较与索引提取-Sequence-Comparison-and-Indexing" class="headerlink" title="序列比较与索引提取(Sequence Comparison and Indexing)"></a>序列比较与索引提取(Sequence Comparison and Indexing)</h4><table>
<thead>
<tr>
<th>操作</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>tf.argmin(input, dimension, name=None)</td>
<td>返回input最小值的索引index</td>
</tr>
<tr>
<td>tf.argmax(input, dimension, name=None)</td>
<td>返回input最大值的索引index</td>
</tr>
<tr>
<td>tf.listdiff(x, y, name=None)</td>
<td>返回x，y中不同值的索引</td>
</tr>
<tr>
<td>tf.where(input, name=None)</td>
<td>返回bool型tensor中为True的位置# ‘input’ tensor is #[[True, False]#[True, False]]# ‘input’ 有两个’True’,那么输出两个坐标值.# ‘input’的rank为2, 所以每个坐标为具有两个维度.where(input) ==&gt;[[0, 0],[1, 0]]</td>
</tr>
<tr>
<td>tf.unique(x, name=None)</td>
<td>返回一个元组tuple(y,idx)，y为x的列表的唯一化数据列表，idx为x数据对应y元素的index# tensor ‘x’ is [1, 1, 2, 4, 4, 4, 7, 8, 8]y, idx = unique(x)y ==&gt; [1, 2, 4, 7, 8]idx ==&gt; [0, 0, 1, 2, 2, 2, 3, 4, 4]</td>
</tr>
<tr>
<td>tf.invert_permutation(x, name=None)</td>
<td>置换x数据与索引的关系# tensor <code>x</code> is [3, 4, 0, 2, 1]invert_permutation(x) ==&gt; [2, 4, 3, 0, 1]</td>
</tr>
</tbody>
</table>
<hr>
<h4 id="神经网络-Neural-Network"><a href="#神经网络-Neural-Network" class="headerlink" title="神经网络(Neural Network)"></a>神经网络(Neural Network)</h4><ul>
<li>激活函数（Activation Functions）</li>
</ul>
<table>
<thead>
<tr>
<th>操作</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>tf.nn.relu(features, name=None)</td>
<td>整流函数：max(features, 0)</td>
</tr>
<tr>
<td>tf.nn.relu6(features, name=None)</td>
<td>以6为阈值的整流函数：min(max(features, 0), 6)</td>
</tr>
<tr>
<td>tf.nn.elu(features, name=None)</td>
<td>elu函数，exp(features) - 1 if &lt; 0,否则features<a href="http://arxiv.org/abs/1511.07289" target="_blank" rel="noopener">Exponential Linear Units (ELUs)</a></td>
</tr>
<tr>
<td>tf.nn.softplus(features, name=None)</td>
<td>计算softplus：log(exp(features) + 1)</td>
</tr>
<tr>
<td>tf.nn.dropout(x, keep_prob, noise_shape=None, seed=None, name=None)</td>
<td>计算dropout，keep_prob为keep概率noise_shape为噪声的shape</td>
</tr>
<tr>
<td>tf.nn.bias_add(value, bias, data_format=None, name=None)</td>
<td>对value加一偏置量此函数为tf.add的特殊情况，bias仅为一维，函数通过广播机制进行与value求和,数据格式可以与value不同，返回为与value相同格式</td>
</tr>
<tr>
<td>tf.sigmoid(x, name=None)</td>
<td>y = 1 / (1 + exp(-x))</td>
</tr>
<tr>
<td>tf.tanh(x, name=None)</td>
<td>双曲线切线激活函数</td>
</tr>
</tbody>
</table>
<ul>
<li>卷积函数（Convolution）</li>
</ul>
<table>
<thead>
<tr>
<th>操作</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>tf.nn.conv2d(input, filter, strides, padding, use_cudnn_on_gpu=None, data_format=None, name=None)</td>
<td>在给定的4D input与 filter下计算2D卷积输入shape为 [batch, height, width, in_channels]</td>
</tr>
<tr>
<td>tf.nn.conv3d(input, filter, strides, padding, name=None)</td>
<td>在给定的5D input与 filter下计算3D卷积输入shape为[batch, in_depth, in_height, in_width, in_channels]</td>
</tr>
</tbody>
</table>
<ul>
<li>池化函数（Pooling）</li>
</ul>
<table>
<thead>
<tr>
<th>操作</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>tf.nn.avg_pool(value, ksize, strides, padding, data_format=’NHWC’, name=None)</td>
<td>平均方式池化</td>
</tr>
<tr>
<td>tf.nn.max_pool(value, ksize, strides, padding, data_format=’NHWC’, name=None)</td>
<td>最大值方法池化</td>
</tr>
<tr>
<td>tf.nn.max_pool_with_argmax(input, ksize, strides,padding, Targmax=None, name=None)</td>
<td>返回一个二维元组(output,argmax),最大值pooling，返回最大值及其相应的索引</td>
</tr>
<tr>
<td>tf.nn.avg_pool3d(input, ksize, strides, padding, name=None)</td>
<td>3D平均值pooling</td>
</tr>
<tr>
<td>tf.nn.max_pool3d(input, ksize, strides, padding, name=None)</td>
<td>3D最大值pooling</td>
</tr>
</tbody>
</table>
<ul>
<li>数据标准化（Normalization）</li>
</ul>
<table>
<thead>
<tr>
<th>操作</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>tf.nn.l2_normalize(x, dim, epsilon=1e-12, name=None)</td>
<td>对维度dim进行L2范式标准化output = x / sqrt(max(sum(x**2), epsilon))</td>
</tr>
<tr>
<td>tf.nn.sufficient_statistics(x, axes, shift=None, keep_dims=False, name=None)</td>
<td>计算与均值和方差有关的完全统计量返回4维元组,<em>元素个数，</em>元素总和，<em>元素的平方和，</em>shift结果<a href="https://www.google.com/url?q=https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Computing_shifted_data&amp;usg=AFQjCNG5RoY7Xvpv4xg-Wy-UJvAPh2zDQw" target="_blank" rel="noopener">参见算法介绍</a></td>
</tr>
<tr>
<td>tf.nn.normalize_moments(counts, mean_ss, variance_ss, shift, name=None)</td>
<td>基于完全统计量计算均值和方差</td>
</tr>
<tr>
<td>tf.nn.moments(x, axes, shift=None, name=None, keep_dims=False)</td>
<td>直接计算均值与方差</td>
</tr>
</tbody>
</table>
<ul>
<li>损失函数（Losses）</li>
</ul>
<table>
<thead>
<tr>
<th>操作</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>tf.nn.l2_loss(t, name=None)</td>
<td>output = sum(t ** 2) / 2</td>
</tr>
</tbody>
</table>
<ul>
<li>分类函数（Classification）</li>
</ul>
<table>
<thead>
<tr>
<th>操作</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>tf.nn.sigmoid_cross_entropy_with_logits(logits, targets, name=None)*</td>
<td>计算输入logits, targets的交叉熵</td>
</tr>
<tr>
<td>tf.nn.softmax(logits, name=None)</td>
<td>计算softmaxsoftmax[i, j] = exp(logits[i, j]) / sum_j(exp(logits[i, j]))</td>
</tr>
<tr>
<td>tf.nn.log_softmax(logits, name=None)</td>
<td>logsoftmax[i, j] = logits[i, j] - log(sum(exp(logits[i])))</td>
</tr>
<tr>
<td>tf.nn.softmax_cross_entropy_with_logits(logits, labels, name=None)</td>
<td>计算logits和labels的softmax交叉熵logits, labels必须为相同的shape与数据类型</td>
</tr>
<tr>
<td>tf.nn.sparse_softmax_cross_entropy_with_logits(logits, labels, name=None)</td>
<td>计算logits和labels的softmax交叉熵</td>
</tr>
<tr>
<td>tf.nn.weighted_cross_entropy_with_logits(logits, targets, pos_weight, name=None)</td>
<td>与sigmoid_cross_entropy_with_logits()相似，但给正向样本损失加了权重pos_weight</td>
</tr>
</tbody>
</table>
<ul>
<li>符号嵌入（Embeddings）</li>
</ul>
<table>
<thead>
<tr>
<th>操作</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>tf.nn.embedding_lookup(params, ids, partition_strategy=’mod’, name=None, validate_indices=True)</td>
<td>根据索引ids查询embedding列表params中的tensor值如果len(params) &gt; 1，id将会安照partition_strategy策略进行分割1、如果partition_strategy为”mod”，id所分配到的位置为p = id % len(params)比如有13个ids，分为5个位置，那么分配方案为：[[0, 5, 10], [1, 6, 11], [2, 7, 12], [3, 8], [4, 9]]2、如果partition_strategy为”div”,那么分配方案为：[[0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 10], [11, 12]]</td>
</tr>
<tr>
<td>tf.nn.embedding_lookup_sparse(params, sp_ids, sp_weights, partition_strategy=’mod’, name=None, combiner=’mean’)</td>
<td>对给定的ids和权重查询embedding1、sp_ids为一个N x M的稀疏tensor，N为batch大小，M为任意，数据类型int642、sp_weights的shape与sp_ids的稀疏tensor权重，浮点类型，若为None，则权重为全’1’</td>
</tr>
</tbody>
</table>
<ul>
<li>循环神经网络（Recurrent Neural Networks）</li>
</ul>
<table>
<thead>
<tr>
<th>操作</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>tf.nn.rnn(cell, inputs, initial_state=None, dtype=None, sequence_length=None, scope=None)</td>
<td>基于RNNCell类的实例cell建立循环神经网络</td>
</tr>
<tr>
<td>tf.nn.dynamic_rnn(cell, inputs, sequence_length=None, initial_state=None, dtype=None, parallel_iterations=None, swap_memory=False, time_major=False, scope=None)</td>
<td>基于RNNCell类的实例cell建立动态循环神经网络与一般rnn不同的是，该函数会根据输入动态展开返回(outputs,state)</td>
</tr>
<tr>
<td>tf.nn.state_saving_rnn(cell, inputs, state_saver, state_name, sequence_length=None, scope=None)</td>
<td>可储存调试状态的RNN网络</td>
</tr>
<tr>
<td>tf.nn.bidirectional_rnn(cell_fw, cell_bw, inputs, initial_state_fw=None, initial_state_bw=None, dtype=None,sequence_length=None, scope=None)</td>
<td>双向RNN, 返回一个3元组tuple(outputs, output_state_fw, output_state_bw)</td>
</tr>
</tbody>
</table>
<blockquote>
<p>— <strong>tf.nn.rnn简要介绍</strong>—<br>cell: 一个RNNCell实例<br>inputs: 一个shape为[batch_size, input_size]的tensor<br>initial_state: 为RNN的state设定初值，可选<br>sequence_length：制定输入的每一个序列的长度，size为[batch_size],值范围为[0, T)的int型数据<br>其中T为输入数据序列的长度<br>@<br>@针对输入batch中序列长度不同，所设置的动态计算机制<br>@对于在时间t，和batch的b行，有<br>(output, state)(b, t) = ? (zeros(cell.output_size), states(b, sequence_length(b) - 1)) : cell(input(b, t), state(b, t - 1))</p>
<hr>
</blockquote>
<ul>
<li>求值网络（Evaluation）</li>
</ul>
<table>
<thead>
<tr>
<th>操作</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>tf.nn.top_k(input, k=1, sorted=True, name=None)</td>
<td>返回前k大的值及其对应的索引</td>
</tr>
<tr>
<td>tf.nn.in_top_k(predictions, targets, k, name=None)</td>
<td>返回判断是否targets索引的predictions相应的值是否在在predictions前k个位置中，返回数据类型为bool类型，len与predictions同</td>
</tr>
</tbody>
</table>
<ul>
<li><a href="https://www.tensorflow.org/versions/r0.10/extras/candidate_sampling.pdf" target="_blank" rel="noopener">监督候选采样网络（Candidate Sampling）</a></li>
</ul>
<p>对于有巨大量的多分类与多标签模型，如果使用全连接softmax将会占用大量的时间与空间资源，所以采用候选采样方法仅使用一小部分类别与标签作为监督以加速训练。</p>
<table>
<thead>
<tr>
<th>操作</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Sampled Loss Functions</strong></td>
<td></td>
</tr>
<tr>
<td>tf.nn.nce_loss(weights, biases, inputs, labels, num_sampled,num_classes, num_true=1, sampled_values=None,remove_accidental_hits=False, partition_strategy=’mod’,name=’nce_loss’)</td>
<td>返回noise-contrastive的训练损失结果</td>
</tr>
<tr>
<td>tf.nn.sampled_softmax_loss(weights, biases, inputs, labels, num_sampled, num_classes, num_true=1, sampled_values=None,remove_accidental_hits=True, partition_strategy=’mod’, name=’sampled_softmax_loss’)</td>
<td>返回sampled softmax的训练损失<a href="http://arxiv.org/pdf/1412.2007.pdf" target="_blank" rel="noopener">参考- Jean et al., 2014第3部分</a></td>
</tr>
<tr>
<td><strong>Candidate Samplers</strong></td>
<td></td>
</tr>
<tr>
<td>tf.nn.uniform_candidate_sampler(true_classes, num_true, num_sampled, unique, range_max, seed=None, name=None)</td>
<td>通过均匀分布的采样集合返回三元tuple1、sampled_candidates 候选集合。2、期望的true_classes个数，为浮点值3、期望的sampled_candidates个数，为浮点值</td>
</tr>
<tr>
<td>tf.nn.log_uniform_candidate_sampler(true_classes, num_true,num_sampled, unique, range_max, seed=None, name=None)</td>
<td>通过log均匀分布的采样集合，返回三元tuple</td>
</tr>
<tr>
<td>tf.nn.learned_unigram_candidate_sampler(true_classes, num_true, num_sampled, unique, range_max, seed=None, name=None)</td>
<td>根据在训练过程中学习到的分布状况进行采样返回三元tuple</td>
</tr>
<tr>
<td>tf.nn.fixed_unigram_candidate_sampler(true_classes, num_true,num_sampled, unique, range_max, vocab_file=”, distortion=1.0, num_reserved_ids=0, num_shards=1, shard=0, unigrams=(), seed=None, name=None)</td>
<td>基于所提供的基本分布进行采样</td>
</tr>
</tbody>
</table>
<h1 id="保存与恢复变量"><a href="#保存与恢复变量" class="headerlink" title="保存与恢复变量"></a>保存与恢复变量</h1><table>
<thead>
<tr>
<th>操作</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>类tf.train.Saver(Saving and Restoring Variables)</td>
<td></td>
</tr>
<tr>
<td>tf.train.Saver.<strong>init</strong>(var_list=None, reshape=False, sharded=False, max_to_keep=5, keep_checkpoint_every_n_hours=10000.0, name=None, restore_sequentially=False,saver_def=None, builder=None)</td>
<td>创建一个存储器Savervar_list定义需要存储和恢复的变量</td>
</tr>
<tr>
<td>tf.train.Saver.save(sess, save_path, global_step=None, latest_filename=None, meta_graph_suffix=’meta’,write_meta_graph=True)</td>
<td>保存变量</td>
</tr>
<tr>
<td>tf.train.Saver.restore(sess, save_path)</td>
<td>恢复变量</td>
</tr>
<tr>
<td>tf.train.Saver.last_checkpoints</td>
<td>列出最近未删除的checkpoint 文件名</td>
</tr>
<tr>
<td>tf.train.Saver.set_last_checkpoints(last_checkpoints)</td>
<td>设置checkpoint文件名列表</td>
</tr>
<tr>
<td>tf.train.Saver.set_last_checkpoints_with_time(last_checkpoints_with_time)</td>
<td>设置checkpoint文件名列表和时间戳</td>
</tr>
</tbody>
</table>

      
      
    </div>
    
  </div>
  
    
    <div class="copyright">
        <p><span>本文标题:</span><a href="/2018/03/20/tensorflow/tensorflow-api-common-func/">《转载》Tensorflow一些常用基本概念与函数</a></p>
        <p><span>文章作者:</span><a href="/" title="访问 Kang 的个人博客">Kang</a></p>
        <p><span>发布时间:</span>2018年03月20日 - 12时20分</p>
        <p><span>最后更新:</span>2018年03月20日 - 12时32分</p>
        <p>
            <span>原始链接:</span><a class="post-url" href="/2018/03/20/tensorflow/tensorflow-api-common-func/" title="《转载》Tensorflow一些常用基本概念与函数">http://blog.kangyufei.net/2018/03/20/tensorflow/tensorflow-api-common-func/</a>
            <span class="copy-path" data-clipboard-text="原文: http://blog.kangyufei.net/2018/03/20/tensorflow/tensorflow-api-common-func/　　作者: Kang" title="点击复制文章链接"><i class="fa fa-clipboard"></i></span>
            <script src="/js/clipboard.min.js"></script>
            <script> var clipboard = new Clipboard('.copy-path'); </script>
        </p>
        <p>
            <span>许可协议:</span><i class="fa fa-creative-commons"></i> <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/3.0/cn/" title="中国大陆 (CC BY-NC-SA 3.0 CN)" target = "_blank">"署名-非商用-相同方式共享 3.0"</a> 转载请保留原文链接及作者。
        </p>
    </div>



<nav id="article-nav">
  
  
    <a href="/2018/03/20/tensorflow/tensorflow-api-const-func/" id="article-nav-older" class="article-nav-link-wrap">
      <div class="article-nav-title">tensorflow API 常量值函数</div>
      <strong class="article-nav-caption">></strong>
    </a>
  
</nav>

  
</article>

    <div id="toc" class="toc-article">
    <strong class="toc-title">文章目录</strong>
    <ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#1、tensorflow的基本运作"><span class="toc-number">1.</span> <span class="toc-text">1、tensorflow的基本运作</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2、tf函数"><span class="toc-number">2.</span> <span class="toc-text">2、tf函数</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#TensorFlow的算术操作如下："><span class="toc-number">2.1.</span> <span class="toc-text">TensorFlow的算术操作如下：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#张量操作Tensor-Transformations"><span class="toc-number">2.2.</span> <span class="toc-text">张量操作Tensor Transformations</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#矩阵相关运算"><span class="toc-number">2.3.</span> <span class="toc-text">矩阵相关运算</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#复数操作"><span class="toc-number">2.4.</span> <span class="toc-text">复数操作</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#归约计算-Reduction"><span class="toc-number">2.5.</span> <span class="toc-text">归约计算(Reduction)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#分割-Segmentation"><span class="toc-number">2.6.</span> <span class="toc-text">分割(Segmentation)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#序列比较与索引提取-Sequence-Comparison-and-Indexing"><span class="toc-number">2.7.</span> <span class="toc-text">序列比较与索引提取(Sequence Comparison and Indexing)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#神经网络-Neural-Network"><span class="toc-number">2.8.</span> <span class="toc-text">神经网络(Neural Network)</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#保存与恢复变量"><span class="toc-number"></span> <span class="toc-text">保存与恢复变量</span></a>
</div>
<input type="button" id="tocButton" value="隐藏目录"  title="点击按钮隐藏或者显示文章目录">

<script src="https://7.url.cn/edu/jslib/comb/require-2.1.6,jquery-1.9.1.min.js"></script>
<script>
    var valueHide = "隐藏目录";
    var valueShow = "显示目录";

    if ($(".left-col").is(":hidden")) {
        $("#tocButton").attr("value", valueShow);
    }
    $("#tocButton").click(function() {
        if ($("#toc").is(":hidden")) {
            $("#tocButton").attr("value", valueHide);
            $("#toc").slideDown(320);
        }
        else {
            $("#tocButton").attr("value", valueShow);
            $("#toc").slideUp(350);
        }
    })
    if ($(".toc").length < 1) {
        $("#toc, #tocButton").hide();
    }
</script>





<div class="bdsharebuttonbox">
	<a href="#" class="fx fa-weibo bds_tsina" data-cmd="tsina" title="分享到新浪微博"></a>
	<a href="#" class="fx fa-weixin bds_weixin" data-cmd="weixin" title="分享到微信"></a>
	<a href="#" class="fx fa-qq bds_sqq" data-cmd="sqq" title="分享到QQ好友"></a>
	<a href="#" class="fx fa-facebook-official bds_fbook" data-cmd="fbook" title="分享到Facebook"></a>
	<a href="#" class="fx fa-twitter bds_twi" data-cmd="twi" title="分享到Twitter"></a>
	<a href="#" class="fx fa-linkedin bds_linkedin" data-cmd="linkedin" title="分享到linkedin"></a>
	<a href="#" class="fx fa-files-o bds_copy" data-cmd="copy" title="分享到复制网址"></a>
</div>
<script>window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"2","bdSize":"24"},"share":{}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];</script>




    
        <div id="gitments"></div>
<script src="/js/gitment.browser.js"></script>
<script>
    var gitment = new Gitment({
      id: window.location.pathname,
      owner: 'kang',
      repo: 'yufeikang.github.io',
      oauth: {
        client_id: '',
        client_secret: '',
      },
    })
    gitment.render('gitments')
</script>
    



    <div class="scroll" id="post-nav-button">
        
            <a href="/" title="回到主页"><i class="fa fa-home"></i></a>
        
        <a title="文章列表"><i class="fa fa-bars"></i><i class="fa fa-times"></i></a>
        
            <a href="/2018/03/20/tensorflow/tensorflow-api-const-func/" title="下一篇: tensorflow API 常量值函数">
                <i class="fa fa-angle-right"></i>
            </a>
        
    </div>
    <ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2018/03/20/tensorflow/tensorflow-api-common-func/">《转载》Tensorflow一些常用基本概念与函数</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/03/20/tensorflow/tensorflow-api-const-func/">tensorflow API 常量值函数</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/03/04/rabbitmq-for-node/">rabbitMQ实践 for Node.js</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/03/01/python/Python装饰器/">Python装饰器</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/10/20/linux/网络嗅探/">网络嗅探</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/10/20/android/android重新打包签名md/">android二进制重新打包</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/06/18/python/Python中的静态成员变量/">Python中的静态成员变量</a></li></ul>
    <script src="https://7.url.cn/edu/jslib/comb/require-2.1.6,jquery-1.9.1.min.js"></script>
    <script>
        $(".post-list").addClass("toc-article");
        $(".post-list-item a").attr("target","_blank");
        $("#post-nav-button > a:nth-child(2)").click(function() {
            $(".fa-bars, .fa-times").toggle();
            $(".post-list").toggle(300);
            if ($(".toc").length > 0) {
                $("#toc, #tocButton").toggle(200, function() {
                    if ($(".switch-area").is(":visible")) {
                        $("#tocButton").attr("value", valueHide);
                        }
                    })
            }
            else {
            }
        })
    </script>



    <script>
        
    </script>
</div>
      <footer id="footer">
    <div class="outer">
        <div id="footer-info">
            <div class="footer-left">
                &copy; 2018 Kang
            </div>
            <div class="footer-right">
                <a href="http://hexo.io/" target="_blank">Hexo</a>  Theme <a href="https://github.com/luuman/hexo-theme-spfk" target="_blank">spfk</a> by luuman
            </div>
        </div>
        
    </div>
</footer>

    </div>
    <script src="https://7.url.cn/edu/jslib/comb/require-2.1.6,jquery-1.9.1.min.js"></script>
<script src="/js/main.js"></script>

    <script>
        $(document).ready(function() {
            var backgroundnum = 24;
            var backgroundimg = "url(/background/bg-x.jpg)".replace(/x/gi, Math.ceil(Math.random() * backgroundnum));
            $("#mobile-nav").css({"background-image": backgroundimg,"background-size": "cover","background-position": "center"});
            $(".left-col").css({"background-image": backgroundimg,"background-size": "cover","background-position": "center"});
        })
    </script>





<div class="scroll" id="scroll">
    <a href="#"><i class="fa fa-arrow-up"></i></a>
    <a href="#comments"><i class="fa fa-comments-o"></i></a>
    <a href="#footer"><i class="fa fa-arrow-down"></i></a>
</div>
<script>
    $(document).ready(function() {
        if ($("#comments").length < 1) {
            $("#scroll > a:nth-child(2)").hide();
        };
    })
</script>

<script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js">
</script>

  <script language="javascript">
    $(function() {
        $("a[title]").each(function() {
            var a = $(this);
            var title = a.attr('title');
            if (title == undefined || title == "") return;
            a.data('title', title).removeAttr('title').hover(

            function() {
                var offset = a.offset();
                $("<div id=\"anchortitlecontainer\"></div>").appendTo($("body")).html(title).css({
                    top: offset.top - a.outerHeight() - 15,
                    left: offset.left + a.outerWidth()/2 + 1
                }).fadeIn(function() {
                    var pop = $(this);
                    setTimeout(function() {
                        pop.remove();
                    }, pop.text().length * 800);
                });
            }, function() {
                $("#anchortitlecontainer").remove();
            });
        });
    });
</script>


  </div>
</body>
</html>